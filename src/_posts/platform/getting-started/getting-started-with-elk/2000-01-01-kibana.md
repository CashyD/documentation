---
title: Kibana
modified_at: 2024-06-03 12:00:00
tags: elk tutorial logstash elasticsearch kibana log
index: 3
---

## Deploying Kibana

### Using our One-Click Deploy Button

Click the One-Click Deploy button below to automatically deploy Kibana with
your Scalingo account:

[![Deploy](https://cdn.scalingo.com/deploy/button.svg)](https://dashboard.scalingo.com/deploy?source=https://github.com/Scalingo/kibana-scalingo)

{% note %}
* When using our One-Click Deploy button above, a new Elasticsearch database is
  provisionned and attached to your Kibana app. You can safely delete it if you
  use the one attached to your Logstash app.
* In such a case, the `ELASTICSEARCH_URL` must be retrieved from the previously
  created Logstash instance.
{% endnote %}

### Using the Command Line

We maintain a repository called [kibana-scalingo](https://github.com/Scalingo/kibana-scalingo)
on GitHub to help you deploy Kibana on Scalingo. Here are the few steps you
will need to follow:

1. Clone our repository:

   ```bash
   git clone https://github.com/Scalingo/kibana-scalingo
   cd kibana-scalingo
   ```

2. Create the application on Scalingo:

   ```bash
   scalingo create my-kibana
   ```

   Notice that our Command Line automatically detects the git repository, and
   adds a git remote to Scalingo:

   ```bash
   git remote -v

   origin   https://github.com/Scalingo/kibana-scalingo (fetch)
   origin   https://github.com/Scalingo/kibana-scalingo (push)
   scalingo git@ssh.osc-fr1.scalingo.com:my-kibana.git (fetch)
   scalingo git@ssh.osc-fr1.scalingo.com:my-kibana.git (push)
   ```

3. Create a few environments variables:

   ```bash
   scalingo --app my-kibana env-set BUILDPACK_URL="https://github.com/Scalingo/kibana-buildpack"
   scalingo --app my-kibana env-set ELASTICSEARCH_URL=""
   ```

4. Deploy:

   ```bash
   git push scalingo master
   ```


## Configuring your Kibana Indexes

Once deployed, index patterns need to be configured. This allows Kibana to know
which indices of Elasticsearch it has to watch.

{% assign img_url = "https://cdn.scalingo.com/documentation/elk/index-creation.png"%}
{% include mdl_img.html %}

In the above example, the `unicorns-*` pattern is used.

Click on create and you're all set, the test input done in the previous section
should appear in the Discover tab of Kibana dashboard.

{% assign img_url = "https://cdn.scalingo.com/documentation/elk/success.png" %}
{% include mdl_img.html %}


## Updating Kibana

By default, Scalingo deploys a version of Kibana that is compatible with the
Elasticsearch instances we provide.

Consequently, updating Kibana only consists in triggering a new deployment of
your instance. To do so, create an empty commit and push it to Scalingo:

```bash
git commit --allow-empty -m "Update Kibana"
git push scalingo master
```

{% note %}
* Scalingo provides a version of Kibana that is compatible with the latest
  Elasticsearch 7.10.x version. Our repository won't be updated as long as we
  are [stuck with this constraint]({% post_url 2000-01-01-overview.md %}).
* However, you can use the dedicated environment variable [see below](#environment)
  to deploy a specific version of your choice.
{% endnote %}

## Customizing your Kibana Deployment

### Environment

The following environment variable(s) can be leveraged to customize your
deployment:

- **`KIBANA_USER`**\
  Username for Kibana authentication.

- **`KIBANA_PASSWORD`**\
  Password for Kibana authentication.

- **`KIBANA_VERSION`**\
  Version of Kibana to deploy.\
  Defaults to `7.10.2`

- **`ELASTICSEARCH_TLS_CA_URL`**\
  URL of the CA certificate used to established a TLS connection with the
  Elasticsearch database. Can be found on the database dashboard.\
  Mandatory if you enabled the force TLS option on your Elasticsearch database.\
  Defaults to not being set.

- **`ES_SSL_VERIFICATION_MODE`**\
  If you're using a custom hostname not handled by the TLS certificate, please
  set this environment variable to `none`.\
  Defaults to not being set.





## Send your application logs to your own ELK stack

One of the multiple usages of the ELK stack is log parsing, storing and
exploration. If you've set up your ELK stack for this, we have a feature called
**log drains** that will automatically send every log line generated by an
application to an ELK stack. Here is [how to add a log drain]({% post_url
platform/app/2000-01-01-log-drain %}) to your application.

When using this configuration, the application name and container index will be
passed in the http query and the message will be in the request body. To parse
this and create meaningful index, you can use the following Logstash configuration (if
your logs are JSON formatted):

```
input {
  http {
    port => "${PORT}"
    user => "${USER}"
    password => "${PASSWORD}"
  }
}

filter {
  grok {
    match => [ "[headers][request_path]", "%{URIPARAM:url}" ]
    remove_field => ["headers"]
  }

  kv {
    source => "url"
    field_split => "&"
    trim_key => "?"
  }

  mutate {
    rename => {
      "appname" => "source"
      "hostname" => "container"
    }
    replace => {
      "host" => "%{source}-%{container}"
    }
  }

  json {
    source => "message"
    target => "msg"
  }
}

output {
  elasticsearch {
    hosts => "${ELASTICSEARCH_HOST}"
    user => "${ELASTICSEARCH_USER}"
    password => "${ELASTICSEARCH_PASSWORD}"
    index => "unicorns-%{+YYYY.MM.dd}"
  }
}
```
